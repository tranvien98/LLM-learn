{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcpVDpVOt8U0zgDWTQnimr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tranvien98/LLM-learn/blob/main/LLM_settings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giải thích các thiết lập của mô hình ngôn ngữ lớn (LLM), bao gồm nhiệt độ (temperature), Top P, độ dài tối đa (max length), dãy dừng (stop sequences), phạt tần suất (frequency penalty) và phạt sự hiện diện (presence penalty). Các thiết lập này ảnh hưởng đến sự ngẫu nhiên, tính đa dạng và độ dài của đầu ra LLM. Hướng dẫn cũng đề cập đến việc không nên thay đổi cả nhiệt độ và Top P hoặc phạt tần suất và phạt sự hiện diện cùng lúc. Cuối cùng, tài liệu liệt kê nhiều ứng dụng và mô hình LLM khác nhau."
      ],
      "metadata": {
        "id": "hklCRo9l7YXp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UffdeevQvk9W",
        "outputId": "0af53e5f-1e6f-4a81-dc20-9f94c179859c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.2.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-ollama)\n",
            "  Downloading langchain_core-0.3.27-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
            "  Downloading ollama-0.4.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (4.12.2)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama<1,>=0.4.4->langchain-ollama)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (1.2.2)\n",
            "Downloading langchain_ollama-0.2.2-py3-none-any.whl (18 kB)\n",
            "Downloading langchain_core-0.3.27-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.5/411.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.4.4-py3-none-any.whl (13 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx, ollama, langchain-core, langchain-ollama\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "Successfully installed httpx-0.27.2 langchain-core-0.3.27 langchain-ollama-0.2.2 ollama-0.4.4\n"
          ]
        }
      ],
      "source": [
        "# install package\n",
        "%pip install -U langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LLM_URL\"] = \"https://a066-34-169-30-144.ngrok-free.app\""
      ],
      "metadata": {
        "id": "lBTF3pGi6nVH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(base_url=os.environ.get(\"LLM_URL\"), model=\"llama3.1:8b\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\"question\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "SoIuhPWp5-oS",
        "outputId": "c838e0fb-833c-43e1-c85e-f251d67421d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To answer what LangChain is, let\\'s break it down into simpler concepts and then build upon those to arrive at a comprehensive understanding.\\n\\n1. **Understanding the Basic Components:**\\n   - The term \"Lang\" in LangChain likely stands for language, indicating that this technology has something to do with linguistic or computational processes related to language.\\n   - The suffix \"-Chain\" implies a series of connected things or events, suggesting a concept that involves a sequence of actions or components linked together.\\n\\n2. **Analyzing the Purpose:**\\n   Given that LangChain involves language and sequences of operations, it likely pertains to natural language processing (NLP) or artificial intelligence (AI). The primary goal in AI is often to create systems that can understand and generate human-like text based on given inputs, such as conversational dialogue.\\n\\n3. **Considering the Potential Applications:**\\n   - In the context of NLP, LangChain might be related to chatbots, language translation software, or text analysis tools.\\n   - It could also relate to the generation of content for various mediums, including blogs, articles, and social media posts, making it a tool in the field of content creation.\\n\\n4. **Recent Advancements:**\\n   The recent advancements in AI, particularly with large language models like BERT, RoBERTa, and GPT-3, have made significant strides in NLP capabilities. These models are complex neural networks that can learn from vast amounts of text data to predict the next word or generate coherent paragraphs.\\n\\n5. **Conclusion:**\\n   Given these considerations, LangChain could be interpreted as a technology that leverages AI and NLP to create advanced language processing systems. This might include software applications for content creation, chatbots with more sophisticated dialogue capabilities, or tools for analyzing vast amounts of text data.\\n\\nPlease note that without direct information on LangChain from its developers or creators, this explanation is based on deduction and the integration of related concepts in AI and NLP.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tham số temperature\n",
        "Tham số này ảnh hướng đến việc chọn token tiếp theo. Trong các mô hình AI thì token tiếp theo luôn được chọn với xác suất cao nhât. Điều chỉnh temperature thêm tính ngẫu nhiên cho việc chọn các token.\n",
        "- Temperature thấp: Tạo ra các kết quả chắc chắn. mô hình luôn chọn các token có xác suất cao nhất.\n",
        "- Temperature cao: Tạo ra kết quả có sự đa dạng hơn. => Thích hợp cho các task cần tính sáng tạo cao.\n",
        "\n",
        "p_i = softmax(x_i/t)\n",
        "\n",
        "công thức gốc: p_i = softmax(x_i)"
      ],
      "metadata": {
        "id": "50ecN6v57nzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "model = OllamaLLM(base_url=os.environ.get(\"LLM_URL\"), model=\"llama3.1:8b\", temperature=0)\n",
        "\n",
        "model.invoke(\"Thực hiện làm bài thơ về Hà Nội khoảng 4 câu?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sV-H8mU-8eLP",
        "outputId": "626bd0e8-b9be-4ddd-970f-38a41fad5e14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hà Nội - thành phố của tình yêu\\nLòng người yêu thương, tràn đầy niềm vui\\nCảnh đẹp như tranh vẽ, không nơi nào sánh được\\nHà Nội - nơi tôi yêu thương nhất trên đời.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Thực hiện làm bài thơ về Hà Nội khoảng 4 câu?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fqY1BFS89C3v",
        "outputId": "3c668a77-4dd8-4890-ab82-cc97782ae904"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hà Nội - thành phố của tình yêu\\nLòng người yêu thương, tràn đầy niềm vui\\nCảnh đẹp như tranh vẽ, không nơi nào sánh được\\nHà Nội - nơi tôi yêu thương nhất trên đời.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = OllamaLLM(base_url=os.environ.get(\"LLM_URL\"), model=\"llama3.1:8b\", temperature=1)\n",
        "\n",
        "model.invoke(\"Thực hiện làm bài thơ về Hà Nội khoảng 4 câu?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xFZk8u9-9E3w",
        "outputId": "bb964fff-4124-4a44-b287-d03e8c402866"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Đường cũ huyền bí\\nVề nhà tò mò đường đến trường\\nTrên phố Bát Đàn\\nSợi sẹo của thời gian còn sót lại \\n\\n(Lời nho đề đựng cảm nhận của tác giả khi đi đến Hà Nội)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Thực hiện làm bài thơ về Hà Nội khoảng 4 câu?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6sCvIRtr9JCX",
        "outputId": "2230d362-6cbf-41c8-df01-f0a32564fcd2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Núi Thăng Long cao vang trập trùng, đằng trước là hồ Tây lớn\\n- Lâu Bờ Động và Văn Miếu cùng đứng yên.\\n- Lá đỏ xuân rơi, phố cũ chìm trong màu tàn\\n- Trong khung cảnh hối hả ồn ào của thành quê.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Nhận xét: Khi temperture=1 thì 2 câu trả lời in ra là khác nhau\n"
      ],
      "metadata": {
        "id": "cj2Foaa-9N-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tham số top_k\n",
        "\n",
        "Để tránh việc tốn chi phí tính toán. sau khi tính softmax chọn ra top k giá trị. Bao gồm 2 quá trình tính softmax và nomalization lại giá trị(vì từ vựng của mô hình ngôn ngữ có thể rất lớn nên tốn chi phí tính toán).\n",
        "\n",
        "Top_k phải nhỏ hơn số lượng từ vựng của model. Nếu top k quá nhỏ thì mô hình trả lời sẽ kém đa dạng."
      ],
      "metadata": {
        "id": "0JMpiUWa9eDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = OllamaLLM(base_url=os.environ.get(\"LLM_URL\"), model=\"llama3.1:8b\", temperature=1, top_k=20)\n",
        "\n",
        "model.invoke(\"Thực hiện làm bài thơ về Hà Nội khoảng 4 câu?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PAC1GpgACRXR",
        "outputId": "702222b2-47c8-46c5-d7a9-7eb5172f0954"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hà Nội xưa nay vẫn là thành phố cổ kính\\nHàng rong và chợ đêm rất nổi tiếng ở Hà Nội\\n\\nBánh bè, bánh cuốn, phở hay mắm tôm đều là những món ăn ngon nổi tiếng.\\n\\nTừ phố cổ đến Hoàn Kiếm, Hà Nội thật sự không thể thiếu.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = OllamaLLM(base_url=os.environ.get(\"LLM_URL\"), model=\"llama3.1:8b\", temperature=1, top_k=5)\n",
        "\n",
        "model.invoke(\"Thực hiện làm bài thơ về Hà Nội khoảng 4 câu?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HEddfjh2DylI",
        "outputId": "46424e0e-242d-4764-c1fd-b0c1dc96549d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Hà Nội - thành phố xưa cũ\\n- Cốm, dừa xiêm là đặc sản của nơi đây\\n- Làng Thợ Đỏ - nơi lưu giữ những truyền thống dân tộc\\n- Hà Nội không thể thiếu phố cổ.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tham số top_p\n",
        "\n",
        "Trong top_p. model tính tổng các xác suất, lấy các giá trị tại nơi mà tổng <= top_p.\n",
        "\n",
        "ví dụ yes:0.6, maybe:0.3, no: 0.1. top_p = 0.9 => yes mà maybe được xem sét\n",
        "\n",
        "top_p không làm giảm quá trình tính toán softmax. Lợi ích  tập hợp vào các từ liên quan đến context nhất.\n",
        "\n",
        "Lưu ý: Không nên sử dụng top_p và temperature cùng 1 lúc."
      ],
      "metadata": {
        "id": "6W_zHfyHEGZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tham số Stopping condition\n",
        "Điều kiện dùng có thể là số lượng token hoặc gặp 1 kí tự nào đó. => Kỹ thuật này làm giảm chi phí tính toán.\n"
      ],
      "metadata": {
        "id": "lD-6FEndGktY"
      }
    }
  ]
}